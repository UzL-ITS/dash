{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db61d2b8",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import onnx\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as tt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import RMSprop, Adam, Adadelta\n",
    "from functools import reduce\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -Uq\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "\n",
    "# MODEL_A\n",
    "MODEL_A_config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"MODEL_A\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=8,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=10,\n",
    ")\n",
    "configs.append(MODEL_A_config)\n",
    "\n",
    "# MODEL_B_POOL_REPL\n",
    "MODEL_B_POOL_REPL_config = dict(\n",
    "    epochs=10,\n",
    "    classes=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"MODEL_B_POOL_REPL\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=9,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=100,\n",
    ")\n",
    "configs.append(MODEL_B_POOL_REPL_config)\n",
    "\n",
    "# MODEL_C\n",
    "MODEL_C_config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"MODEL_C\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=9,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=100,\n",
    ")\n",
    "configs.append(MODEL_C_config)\n",
    "\n",
    "# MODEL_D_POOL_REPL\n",
    "MODEL_D_POOL_REPL_config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"MODEL_D_POOL_REPL\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=8,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=100,\n",
    ")\n",
    "configs.append(MODEL_D_POOL_REPL_config)\n",
    "\n",
    "# MODEL_E_30\n",
    "MODEL_E_30_config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"MODEL_E_30\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=5,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=100,\n",
    ")\n",
    "configs.append(MODEL_E_30_config)\n",
    "\n",
    "# MODEL_E_100\n",
    "MODEL_E_100_config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"MODEL_E_100\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=5,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=100,\n",
    ")\n",
    "configs.append(MODEL_E_100_config)\n",
    "\n",
    "# MODEL_F_MINIONN_POOL_REPL\n",
    "MODEL_F_MINIONN_POOL_REPL_config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"CIFAR10\",\n",
    "    architecture=\"MODEL_F_MINIONN_POOL_REPL\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=9,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=10,\n",
    ")\n",
    "configs.append(MODEL_F_MINIONN_POOL_REPL_config)\n",
    "\n",
    "# MODEL_F_GNNP_POOL_REPL\n",
    "MODEL_F_GNNP_POOL_REPL_config = dict(\n",
    "    epochs=100,\n",
    "    classes=10,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    optimizer=\"Adam\",\n",
    "    dataset=\"CIFAR10\",\n",
    "    architecture=\"MODEL_F_GNNP_POOL_REPL\",\n",
    "    fake_quantization_const=0,\n",
    "    fake_quantize_weights=False,\n",
    "    target_crt_base_size=9,\n",
    "    test_size=10000,\n",
    "    quantize_set_size=10,\n",
    ")\n",
    "configs.append(MODEL_F_GNNP_POOL_REPL_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff62332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    with wandb.init(project=\"dash\", config=hyperparameters, reinit=True):\n",
    "        config = wandb.config\n",
    "\n",
    "        model, train_loader, test_loader, val_loader, criterion, optimizer = make(\n",
    "            config\n",
    "        )\n",
    "        print(model)\n",
    "        best_model = train(\n",
    "            model, train_loader, test_loader, val_loader, criterion, optimizer, config\n",
    "        )\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44041cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    train, test, val = get_data(config.dataset)\n",
    "    train_loader = DataLoader(\n",
    "        train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test, batch_size=config.batch_size, pin_memory=True, num_workers=8\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val, batch_size=config.batch_size, pin_memory=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    architecure = globals()[config.architecture]\n",
    "    model = architecure(config.fake_quantization_const).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = globals()[config.optimizer](\n",
    "        model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    return model, train_loader, test_loader, val_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_name):\n",
    "    if dataset_name == \"MNIST\":\n",
    "        training_data = datasets.MNIST(\n",
    "            root=\"../../data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=tt.ToTensor(),\n",
    "        )\n",
    "\n",
    "        # Download test data from open datasets.\n",
    "        test_data = datasets.MNIST(\n",
    "            root=\"../../data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=tt.ToTensor(),\n",
    "        )\n",
    "\n",
    "        training_data, val_data = torch.utils.data.random_split(\n",
    "            training_data, [55000, 5000]\n",
    "        )\n",
    "\n",
    "        return training_data, test_data, val_data\n",
    "\n",
    "    if dataset_name == \"CIFAR10\":\n",
    "        stats = ((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "        train_tfms = tt.Compose(\n",
    "            [\n",
    "                tt.RandomCrop(32, padding=4, padding_mode=\"reflect\"),\n",
    "                tt.RandomHorizontalFlip(),\n",
    "                tt.ToTensor(),\n",
    "                tt.Normalize(*stats, inplace=True),\n",
    "            ]\n",
    "        )\n",
    "        test_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
    "\n",
    "        training_data = datasets.CIFAR10(\n",
    "            root=\"../../data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=train_tfms,\n",
    "        )\n",
    "\n",
    "        # Download test data from open datasets.\n",
    "        test_data = datasets.CIFAR10(\n",
    "            root=\"../../data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=test_tfms,\n",
    "        )\n",
    "\n",
    "        training_data, val_data = torch.utils.data.random_split(\n",
    "            training_data, [45000, 5000]\n",
    "        )\n",
    "\n",
    "        return training_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantization(nn.Module):\n",
    "    def __init__(self, fake_quantization_const):\n",
    "        super(FakeQuantization, self).__init__()\n",
    "        self.fake_quantization_const = fake_quantization_const\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            torch.round(x / self.fake_quantization_const) * self.fake_quantization_const\n",
    "        )\n",
    "\n",
    "    def backward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385839a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_A(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_A, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MODEL_B_POOL_REPL(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_B_POOL_REPL, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 5, kernel_size=3, stride=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 10, kernel_size=3, stride=3),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * 2 * 10, 100),  # upscaling?!\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MODEL_C(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_C, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(13 * 13 * 5, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MODEL_D_POOL_REPL(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_D_POOL_REPL, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MODEL_E_30(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_E_30, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 30),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(30, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MODEL_E_100(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_E_100, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MODEL_F_MINIONN_POOL_REPL(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_F_MINIONN_POOL_REPL, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3),  # 1\n",
    "            nn.ReLU(),  # 2\n",
    "            nn.Conv2d(64, 64, kernel_size=3),  # 3\n",
    "            nn.ReLU(),  # 4\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=2),  # replaces maxpool\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),  # 6\n",
    "            nn.ReLU(),  # 7\n",
    "            nn.Conv2d(64, 64, kernel_size=3),  # 8\n",
    "            nn.ReLU(),  # 9\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=2),  # replaces maxpool\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),  # 11\n",
    "            nn.ReLU(),  # 12\n",
    "            nn.Conv2d(64, 64, kernel_size=1),  # 13\n",
    "            nn.ReLU(),  # 14\n",
    "            nn.Conv2d(64, 16, kernel_size=1),  # 15\n",
    "            nn.ReLU(),  # 16\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(16 * 3 * 3, 10),  # 17\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MODEL_F_GNNP_POOL_REPL(nn.Module):\n",
    "    def __init__(self, fake_quantization_const=0):\n",
    "        super(MODEL_F_GNNP_POOL_REPL, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2),  # replaces maxpool\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=2),  # replaces maxpool\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b082555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, fake_quantization_const):\n",
    "    for p in model.parameters():\n",
    "        p.data = torch.round(p.data / fake_quantization_const) * fake_quantization_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, val_loader, criterion, optimizer, config):\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    size = len(train_loader.dataset)\n",
    "\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_model_val_loss, best_model_val_acc = test(best_model, val_loader, criterion)\n",
    "    example_cnt = 0\n",
    "\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        if epoch > 75:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = 0.0005\n",
    "        if epoch > 100:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = 0.0003\n",
    "\n",
    "        train_loss, correct = 0, 0\n",
    "        for _, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            example_cnt += len(X)\n",
    "\n",
    "            # Quantize parameter during training\n",
    "            if config.fake_quantization_const != 0 and config.fake_quantize_weights:\n",
    "                quantize_model(model, config.fake_quantization_const)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            _, pred_label = torch.max(pred.data, 1)\n",
    "            correct += (pred_label == y).sum().item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Report training_loss\n",
    "            wandb.log({\"train\": {\"loss\": train_loss}}, step=example_cnt)\n",
    "\n",
    "        # Report train accuracy and validation metrcis\n",
    "        train_acc = correct / size\n",
    "        if config.fake_quantization_const != 0 and config.fake_quantize_weights:\n",
    "            quantize_model(model, config.fake_quantization_const)\n",
    "\n",
    "        val_loss, val_acc = test(model, val_loader, criterion)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train\": {\"acc\": train_acc},\n",
    "                \"val\": {\"loss\": val_loss, \"acc\": val_acc},\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=example_cnt,\n",
    "        )\n",
    "\n",
    "        # Save best model with respect to validation loss\n",
    "        if val_loss < best_model_val_loss:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_val_loss = val_loss\n",
    "            best_model_val_acc = val_acc\n",
    "            best_model_train_loss = train_loss\n",
    "            best_model_train_acc = train_acc\n",
    "            wandb.run.summary[\"best_model_val_loss\"] = best_model_val_loss\n",
    "            wandb.run.summary[\"best_model_val_acc\"] = best_model_val_acc\n",
    "            wandb.run.summary[\"best_model_val_loss\"] = best_model_train_loss\n",
    "            wandb.run.summary[\"best_model_val_acc\"] = best_model_train_acc\n",
    "            wandb.run.summary[\"best_epoch\"] = epoch\n",
    "\n",
    "    # Get test metrics\n",
    "    test_loss, test_acc = test(best_model, test_loader, criterion)\n",
    "    wandb.run.summary[\"best_model_test_loss\"] = test_loss\n",
    "    wandb.run.summary[\"best_model_test_acc\"] = test_acc\n",
    "    # Serialize model\n",
    "    serialize_model(best_model, train_loader)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ebd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    size = len(test_loader.dataset)\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += criterion(pred, y).item()\n",
    "\n",
    "            _, pred_label = torch.max(pred.data, 1)\n",
    "            correct += (pred_label == y).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_accuracy = correct / size\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model(model, loader):\n",
    "    dir_path = \"../trained_models/\"\n",
    "    file_name = model.__class__.__name__ + \".onnx\"\n",
    "    model_path = dir_path + file_name\n",
    "\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images.to(device)\n",
    "\n",
    "    input_names = [\"actual_input_1\"]\n",
    "    output_names = [\"output1\"]\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        images,\n",
    "        file_name,\n",
    "        verbose=True,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        operator_export_type=torch.onnx.OperatorExportTypes.ONNX_FALLTHROUGH,\n",
    "    )\n",
    "    # Check that the model is well formed\n",
    "    # onnx_model = onnx.load(file_name)\n",
    "    # onnx.checker.check_model(onnx_model)\n",
    "    wandb.save(file_name)\n",
    "    os.popen(f\"cp {file_name} {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0308e99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for config in configs:\n",
    "    best_model = model_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
